{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QJvC8Pbn7jvo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/9a/06/e4ec2a321e57c03b7e9345d709d554a52c33760e5015fdff0919d9459af0/transformers-4.35.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.17.3 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/7c/55/b3432f43d6d7fee999bb23a547820d74c48ec540f5f7842e41aa5d8d5f3a/datasets-2.14.6-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/35/a8/36d8d7b3e46b377800d8dec47891cdf05842d1a2366909ae4a0c89fbc5e6/multiprocess-0.70.15-py310-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
      "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15 xxhash-3.4.1\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Collecting pythonnet\n",
      "  Obtaining dependency information for pythonnet from https://files.pythonhosted.org/packages/12/28/2ecca48b313c436eb0b194b74b5a70dd88f2700d79bce5424799ffc82ede/pythonnet-3.0.3-py3-none-any.whl.metadata\n",
      "  Downloading pythonnet-3.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting clr-loader<0.3.0,>=0.2.6 (from pythonnet)\n",
      "  Obtaining dependency information for clr-loader<0.3.0,>=0.2.6 from https://files.pythonhosted.org/packages/b3/1a/2801c4e3888aac8fcd4d88d32a1155a778d1c01ca040ae8137d433a39cc6/clr_loader-0.2.6-py3-none-any.whl.metadata\n",
      "  Downloading clr_loader-0.2.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: cffi>=1.13 in /opt/conda/lib/python3.10/site-packages (from clr-loader<0.3.0,>=0.2.6->pythonnet) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.13->clr-loader<0.3.0,>=0.2.6->pythonnet) (2.21)\n",
      "Downloading pythonnet-3.0.3-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading clr_loader-0.2.6-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: clr-loader, pythonnet\n",
      "Successfully installed clr-loader-0.2.6 pythonnet-3.0.3\n",
      "Collecting psycopg2-binary\n",
      "  Obtaining dependency information for psycopg2-binary from https://files.pythonhosted.org/packages/bc/0d/486e3fa27f39a00168abfcf14a3d8444f437f4b755cc34316da1124f293d/psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (2.0.21)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install nltk\n",
    "!pip install sentencepiece\n",
    "!pip install pythonnet\n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISvbC0Lq5pbn"
   },
   "source": [
    "## **Text analytics VM code**\n",
    "\n",
    "- Clean data: auto correct and other preprocessing steps\n",
    "- Overal sentiment classifier\n",
    "- Emotion detector\n",
    "- Name masking and location extractor\n",
    "- Aspect based sentiment prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOQu62fj6R4X"
   },
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U0JveDUrx5N6"
   },
   "outputs": [],
   "source": [
    "#packages\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#finding the terms in the response\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download()\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "max_len = 430\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYpDNoGt6seQ"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Q9FUaGnW6qJq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xS_LaTck52VG"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "def postgresql_to_dataframe(conn, select_query, column_names):\n",
    "    \"\"\"\n",
    "    Transform a SELECT query result into a pandas DataFrame.\n",
    "\n",
    "    :param conn: A PostgreSQL database connection object.\n",
    "    :param select_query: The SQL SELECT query to execute.\n",
    "    :param column_names: A list of column names for the resulting DataFrame.\n",
    "    :return: A pandas DataFrame containing the query results.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Execute the SELECT query using the provided connection\n",
    "        cursor.execute(select_query)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        # Handle any query execution errors, print the error message, and close the cursor\n",
    "        print(\"Error: %s\" % error)\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    # Fetch all the query results into a list of tuples\n",
    "    tupples = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    # Create a pandas DataFrame from the list of tuples with the specified column names\n",
    "    df = pd.DataFrame(tupples, columns=column_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def connect(params_dic):\n",
    "    \"\"\"\n",
    "    Connect to the PostgreSQL database server using the provided connection parameters.\n",
    "        :param params_dic: A dictionary containing database connection parameters.\n",
    "        :return: A database connection object.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # Attempt to connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**params_dic)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        # Handle any connection errors, print the error message, and exit\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    print(\"Connection successful\")\n",
    "\n",
    "    # Return the database connection object\n",
    "    return conn\n",
    "\n",
    "\n",
    "\n",
    "def extract(query, param_dic):\n",
    "    # Passing the connection details for the PostgreSQL server\n",
    "    # Connect to the database using the provided connection parameters\n",
    "    conn = connect(param_dic)\n",
    "    # Execute the provided SQL query and retrieve the results as a DataFrame\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    # Print a message to indicate that the data extraction is complete\n",
    "    print(\"Data extract complete...\")\n",
    "    # Return the DataFrame containing the extracted data\n",
    "    return df\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    # Get a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    # Use a cursor for executing SQL commands\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        # Create a StringIO object to store the data in a CSV format\n",
    "        s_buf = StringIO()\n",
    "        # Create a CSV writer to write the data to the StringIO buffer\n",
    "        writer = csv.writer(s_buf)\n",
    "        # Write the data_iter (the data rows) to the StringIO buffer\n",
    "        writer.writerows(data_iter)\n",
    "        # Reset the buffer position to the beginning\n",
    "        s_buf.seek(0)\n",
    "        # Generate the list of column names as a comma-separated string\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        # Determine the full table name, including schema if it exists\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "        # Construct the SQL query for data insertion using the PostgreSQL COPY command\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        # Execute the COPY command with the data from the StringIO buffer\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "\n",
    "\n",
    "def close_connection(param_dic):\n",
    "    # Establish a connection to the PostgreSQL database using the provided parameters.\n",
    "    conn = psycopg2.connect(database=param_dic['database'],\n",
    "                            user=param_dic['user'],\n",
    "                            password=param_dic['password'],\n",
    "                            host=param_dic['host'],\n",
    "                            port=\"5432\")\n",
    "\n",
    "    # Print the 'closed' attribute of the connection (0 means open, 1 means closed).\n",
    "    print(conn.closed)\n",
    "    # Close the database connection to release resources.\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def upload(df, param_dic):\n",
    "    # Print a message to indicate the upload process has started.\n",
    "    print('Uploading to PostgreSQL...')\n",
    "    # Create a database engine using the provided connection parameters.\n",
    "    engine = create_engine('postgresql://' + param_dic['user'] + ':' + param_dic['password'] + '@' + param_dic['host'] + ':5432/' + param_dic['database'])\n",
    "    # Upload the DataFrame 'df' to the PostgreSQL database using the provided engine.\n",
    "    ''' - 'a_test' is the table name in the database.\n",
    "        - 'sandbox' is the schema where the table will be created.\n",
    "        - if_exists='replace' will replace the table if it already exists.\n",
    "        - index=False indicates not to include the DataFrame index in the database.'''\n",
    "    df.to_sql('a_test', engine, method=psql_insert_copy, schema='sandbox', if_exists='replace', index=False)\n",
    "    # Close the database connection using a separate function.\n",
    "    close_connection(param_dic)\n",
    "    # Print a message to indicate that the database connection has been closed.\n",
    "    print('Connection closed.')\n",
    "\n",
    "\n",
    "\n",
    "def run_query(query, param_dic):\n",
    "    # Establish a connection to the PostgreSQL database using the provided parameters.\n",
    "    conn = psycopg2.connect(database=param_dic['database'],\n",
    "                            user=param_dic['user'],\n",
    "                            password=param_dic['password'],\n",
    "                            host=param_dic['host'],\n",
    "                            port=\"5432\")\n",
    "\n",
    "    # Create a cursor object to interact with the database.\n",
    "    cursor = conn.cursor()\n",
    "    # Execute the SQL query provided as a parameter.\n",
    "    cursor.execute(query)\n",
    "    # Commit the transaction to save the changes to the database.\n",
    "    cursor.execute(\"COMMIT\")\n",
    "    # Close the database connection to release resources.\n",
    "    conn.close()\n",
    "\n",
    "def concat_csv_files(directory, extension='csv'):\n",
    "    \"\"\"\n",
    "    Concatenate multiple CSV files in a directory into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "        extension (str): File extension of the CSV files (default is 'csv').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame.\n",
    "\n",
    "    # Specify the directory and extension\n",
    "        directory_path = \"G:/.shortcut-targets-by-id/1uyENisO-y_QgU7JTbN4TWyLtow8ngXtZ/PJS/Coach\"\n",
    "        file_extension = 'csv'\n",
    "    # Call the function to concatenate CSV files\n",
    "        df = concat_csv_files(directory_path, file_extension)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)  # Change the working directory to the specified directory\n",
    "    all_filenames = [i for i in glob.glob(f'*.{extension}')]  # List of all CSV files\n",
    "\n",
    "    # Concatenate all CSV files into a single DataFrame\n",
    "    df = pd.concat([pd.read_csv(f, encoding='unicode_escape') for f in all_filenames], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahuOLynnApXT"
   },
   "source": [
    "### Overall sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tI1LH4InAt_j"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on CPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = 200\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    #defining the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=494,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 3\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', torchscript = True)\n",
    "\n",
    "\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "\n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "def download_model_weight(bucket_name, source_blob_name, destination_file_path):\n",
    "    \"\"\"Downloads a model weight file from Google Cloud Storage.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Download the model weight file\n",
    "    blob.download_to_filename(destination_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"sentiment_response\"\n",
    "    source_blob_name = r\"{SERIES}/{EXPERIMENT}/models/{TIMESTAMP}/model/sentiment_classifier.pt\"  # Path to the model weight file in the bucket\n",
    "    destination_file_path = \"model_weights.pt\"  # Local path where you want to save the model weight file\n",
    "\n",
    "    download_model_weight(bucket_name, source_blob_name, destination_file_path)\n",
    "\n",
    "def overal_sentiment(df):\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "\n",
    "  path = r'model_weights.pt'\n",
    "  bert_classifier.load_state_dict(torch.load(path))\n",
    "\n",
    "  test_inputs, test_masks = preprocessing_for_bert(df['response'].to_numpy())\n",
    "\n",
    "  # Create the DataLoader for our test set\n",
    "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "  # Compute predicted probabilities on the test set\n",
    "  probs = bert_predict(bert_classifier, test_dataloader)\n",
    "  classification = np.argmax(probs, axis = 1)\n",
    "\n",
    "  return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iQlq-xQDfy6"
   },
   "source": [
    "### Emotion detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uVEaKtzaDiGM"
   },
   "outputs": [],
   "source": [
    "def emotion_detector(df):\n",
    "  prob_df = df['response'].to_frame()\n",
    "  ds = Dataset.from_pandas(prob_df)\n",
    "\n",
    "  pipe = pipeline('text-classification', model=\"j-hartmann/emotion-english-distilroberta-base\", device = device)\n",
    "  results = pipe(KeyDataset(ds, \"response\"))\n",
    "\n",
    "  emotion_label = []\n",
    "  for idx, extracted_entities in enumerate(results):\n",
    "    emotion_label.append(extracted_entities['label'])\n",
    "\n",
    "  return emotion_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERy7GUzLRjtv"
   },
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Dcnf8TdER7TJ"
   },
   "outputs": [],
   "source": [
    "def ner(df):\n",
    "  prob_df = df['response'].to_frame()\n",
    "  ds = Dataset.from_pandas(prob_df)\n",
    "\n",
    "  model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "  pipe = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, device = device)\n",
    "  results = pipe(KeyDataset(ds, \"response\"))\n",
    "\n",
    "  original = []\n",
    "  temp_ent = []\n",
    "  entities = []\n",
    "\n",
    "  for idx, extracted_entities in enumerate(results):\n",
    "      temp_ent = []\n",
    "      # print('idx: ', idx)\n",
    "      # print(\"Original text:\\n{}\".format(ds[idx][\"response\"]))\n",
    "      # print(\"Extracted entities:\")\n",
    "      #original.append(dataset[idx][\"text\"])\n",
    "      for entity in extracted_entities:\n",
    "          temp_ent.append(entity)\n",
    "          # print(entity)\n",
    "      entities.append(temp_ent)\n",
    "\n",
    "  return entities\n",
    "\n",
    "def extraction(df):\n",
    "  df['entities'] = ner(df)\n",
    "  df2 = df.explode('entities')\n",
    "\n",
    "  df1 = df2[df2['entities'].isna() == True]\n",
    "  df2 = df2[df2['entities'].isna() == False]\n",
    "\n",
    "  #entity\n",
    "  df2['entity_type'] = df2['entities'].apply(lambda x:  x['entity'])\n",
    "  #start\n",
    "  df2['entity_start'] = df2['entities'].apply(lambda x:  x['start'])\n",
    "  #end\n",
    "  df2['entity_end'] = df2['entities'].apply(lambda x:  x['end'])\n",
    "  #word\n",
    "  df2['entity_word'] = df2['entities'].apply(lambda x:  x['word'])\n",
    "\n",
    "  df['Name_mask'] = np.where( df['ticket'].isin( df2[df2['entity_type'] == 'B-PER'] ), True, False )\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://absa-classification/absa/config.json...\n",
      "Copying gs://absa-classification/absa/model.safetensors...                      \n",
      "| [2 files][  1.6 GiB/  1.6 GiB]   49.8 MiB/s                                   \n",
      "Operation completed over 2 objects/1.6 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r gs://absa-classification/absa ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFNdnSRfXvX0"
   },
   "source": [
    "### Aspect based sentiment classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "i58dRFF-Xxxz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# cleaning master function\n",
    "def clean_response(response, bigrams=False):\n",
    "    response = str(response).lower() # lower case\n",
    "    response = re.sub('['+my_punctuation + ']+', ' ', response) # strip punctuation\n",
    "    response = re.sub('\\s+', ' ', response) #remove double spacing\n",
    "    response = re.sub('([0-9]+)', '', response) # remove numbers\n",
    "    response_token_list = [word for word in response.split(' ')]\n",
    "                           # if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    response_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in response_token_list] # apply word rooter\n",
    "\n",
    "    lem_response = ' '.join(response_token_list)\n",
    "    return response, lem_response\n",
    "\n",
    "\n",
    "#lemmatizing taxonomy\n",
    "def lem_taxon(term):\n",
    "    term = str(term).lower()\n",
    "    term = word_rooter(term)\n",
    "    return term\n",
    "def match_both(response, non_lem, tax_list, tier2_list, tier1_list):\n",
    "    tier_match = []\n",
    "    tier_replace = ['replace', 'this', 'part']\n",
    "    words = response.split(' ')\n",
    "    non_lem_words = non_lem.split(' ')\n",
    "    for i, x in enumerate(tax_list):\n",
    "        for n,y in enumerate(words):\n",
    "          if x == y:\n",
    "            tier_match.append([non_lem_words[n], tier2_list[i], tier1_list[i]])\n",
    "\n",
    "    tier_match = [list(x) for x in set(tuple(x) for x in tier_match)]\n",
    "    return tier_match\n",
    "\n",
    "def apply_taxonomy(df):\n",
    "  df['tax_match'] = df.apply(lambda x: match_both(x['lem_response'], x['non_lem_response'], tax['Term'].to_list(), tax['Tier 2'].to_list(), tax['Tier 1'].to_list() ) , axis = 1)\n",
    "  df = df.explode('tax_match')\n",
    "\n",
    "  df['tax_match'] = df['tax_match'].fillna('No Taxonomy')\n",
    "  import math\n",
    "  #unpacking tax_match arrays into individual columns\n",
    "  tax_match = df['tax_match'].to_list()\n",
    "\n",
    "  term = []\n",
    "  tier2 = []\n",
    "  tier1 = []\n",
    "  clause = []\n",
    "\n",
    "  for i, c in enumerate(tax_match):\n",
    "    if c != 'No Taxonomy':\n",
    "      term.append(c[0])\n",
    "      tier2.append(c[1])\n",
    "      tier1.append(c[2])\n",
    "    else:\n",
    "      term.append(np.nan)\n",
    "      tier2.append(np.nan)\n",
    "      tier1.append(np.nan)\n",
    "\n",
    "  df['Term'], df['Tier_2'], df['Tier_1']= term, tier2, tier1\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0W0hh29ZFvz"
   },
   "source": [
    "### loading absa model for batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "PiCpm4qEjmMK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example):\n",
    "    checkpoint = \"yangheng/deberta-v3-large-absa-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "    return tokenizer(example[\"response\"], example[\"Term\"], truncation=True)\n",
    "\n",
    "def absa_classification(df):\n",
    "  #packaging data appropriately\n",
    "  absa = df.dropna(subset = 'Term')\n",
    "  absa['Term'] = absa['Term'].apply(lambda x: text_preprocessing(x))\n",
    "  predict_ds = Dataset.from_pandas(absa[['response','Term']])\n",
    "\n",
    "  model_save_name = 'absa'\n",
    "    # Load your fine-tuned model\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_save_name)\n",
    "\n",
    "  checkpoint = \"yangheng/deberta-v3-large-absa-v1.1\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "  #tokenizing dataset\n",
    "  tokenized_datasets = predict_ds.map(tokenize_function, batched=True)\n",
    "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "  #removing unnecessary columns\n",
    "  tokenized_datasets = tokenized_datasets.remove_columns([\"response\", \"Term\", \"__index_level_0__\"])\n",
    "  tokenized_datasets.set_format(\"torch\")\n",
    "  tokenized_datasets.column_names\n",
    "\n",
    "  predict_dataloader = DataLoader(\n",
    "      tokenized_datasets, shuffle=True, batch_size=8, collate_fn=data_collator\n",
    "  )\n",
    "\n",
    "  for batch in predict_dataloader:\n",
    "      break\n",
    "  {k: v.shape for k, v in batch.items()}\n",
    "\n",
    "\n",
    "\n",
    "  #batch predictions\n",
    "  model.to(device)\n",
    "  all_logits = []\n",
    "  predictions = []\n",
    "  for batch in predict_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**batch)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      all_logits.append(logits)\n",
    "\n",
    "      prediction = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "      predictions.extend(prediction)\n",
    "\n",
    "  absa['term_sentiment'] = predictions\n",
    "  df = df.merge( absa[['ticket','Term','term_sentiment']], how = 'left', left_on = ['ticket','Term'], right_on = ['ticket','Term'] )\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GA1vhgIDq_fA"
   },
   "outputs": [],
   "source": [
    "\n",
    "#loading taxonomy\n",
    "taxonomy_path = r'gs://absa-classification/taxonomy.csv' \n",
    "tax = pd.read_csv(taxonomy_path)\n",
    "#clean reason response\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "\n",
    "\n",
    "tax['Term'] = tax['Term'].apply(lambda x: lem_taxon(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'gs://sentiment_response/pjs_snippy.csv', nrows = 200)\n",
    "df = df.dropna()\n",
    "\n",
    "df['nps'] = df['nps'].replace('10 (Extremely likely)',10)\n",
    "df['nps'] = df['nps'].replace('0 (Not at all likely)',0)\n",
    "df['nps'] = df['nps'].astype(int)\n",
    "\n",
    "#target variable will nps split into demoters, passives and promoters\n",
    "df['label'] = np.where(df['nps'] == 3,2,\n",
    "                  np.where(df['nps'] == 1,0,1))\n",
    "\n",
    "df['response'] = df['response'].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMGEwTxsrRea",
    "outputId": "2a88ee79-22e6-4d3d-9f32-fdabf8f219cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "prob_df = df['response'].to_frame()\n",
    "\n",
    "ds = Dataset.from_pandas(prob_df)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "K6286yvWrXBY"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "xKhpY2pzt88S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/var/tmp/ipykernel_12214/573941777.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  absa['Term'] = absa['Term'].apply(lambda x: text_preprocessing(x))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea9779407074d82a392bb360c739458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/var/tmp/ipykernel_12214/573941777.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  absa['term_sentiment'] = predictions\n"
     ]
    }
   ],
   "source": [
    "df['Overall sentiment'] = overal_sentiment(df)\n",
    "df['emotion'] = emotion_detector(df)\n",
    "\n",
    "#Using regex to mask phone numbers and emails\n",
    "#UK phone number\n",
    "phone_pattern = '((\\+44(\\s\\(0\\)\\s|\\s0\\s|\\s)?)|0)7\\d{3}(\\s)?\\d{6}'\n",
    "df['masked_response'] = df['response'].apply(lambda x: re.sub(phone_pattern, '[number_removed]',x ))\n",
    "\n",
    "#email address\n",
    "email_pattern = r'([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\\.[A-Z|a-z]{2,})+'\n",
    "df['masked_response'] = df['response'].apply(lambda x: re.sub(email_pattern, '[email_removed]',x ))\n",
    "\n",
    "df = extraction(df)\n",
    "\n",
    "df['non_lem_response'],df['lem_response'] = zip(*df['response'].apply(lambda x: clean_response(x)))\n",
    "df = apply_taxonomy(df)\n",
    "\n",
    "\n",
    "df = absa_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8030329 entries, 0 to 8030328\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   ticket             object \n",
      " 1   nps                int64  \n",
      " 2   response           object \n",
      " 3   csat               int64  \n",
      " 4   date               object \n",
      " 5   length             int64  \n",
      " 6   label              int64  \n",
      " 7   Overall sentiment  int64  \n",
      " 8   emotion            object \n",
      " 9   masked_response    object \n",
      " 10  entities           object \n",
      " 11  Name_mask          bool   \n",
      " 12  non_lem_response   object \n",
      " 13  lem_response       object \n",
      " 14  tax_match          object \n",
      " 15  Term               object \n",
      " 16  Tier_2             object \n",
      " 17  Tier_1             object \n",
      " 18  term_sentiment     float64\n",
      "dtypes: bool(1), float64(1), int64(5), object(12)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkZThDfg4uHd",
    "outputId": "f5d6673b-9142-40c6-9b1c-e9c0c67712e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to PostgreSQL...\n",
      "0\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_dic = {\n",
    "        \"host\"      : \"ec2-54-155-12-138.eu-west-1.compute.amazonaws.com\",\n",
    "        \"database\"  : \"d53ibcjuk3k509\",\n",
    "        \"user\"      : \"arun_nagrecha_nationalexpress_com\",\n",
    "        \"password\"  : \"p77cef9d11a8b16cc2e52423659dcc3e2d86393550ea7e0b6ac6b84050747d3b0\"\n",
    "    }\n",
    "\n",
    "upload(df,param_dic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
