{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJvC8Pbn7jvo"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install nltk\n",
    "!pip install sentencepiece\n",
    "!pip install pythonnet\n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISvbC0Lq5pbn"
   },
   "source": [
    "## **Text analytics VM code**\n",
    "\n",
    "- Clean data: auto correct and other preprocessing steps\n",
    "- Overal sentiment classifier\n",
    "- Emotion detector\n",
    "- Name masking and location extractor\n",
    "- Aspect based sentiment prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOQu62fj6R4X"
   },
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U0JveDUrx5N6"
   },
   "outputs": [],
   "source": [
    "#packages\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#finding the terms in the response\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download()\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "max_len = 430\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYpDNoGt6seQ"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q9FUaGnW6qJq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xS_LaTck52VG"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "def postgresql_to_dataframe(conn, select_query, column_names):\n",
    "    \"\"\"\n",
    "    Transform a SELECT query result into a pandas DataFrame.\n",
    "\n",
    "    :param conn: A PostgreSQL database connection object.\n",
    "    :param select_query: The SQL SELECT query to execute.\n",
    "    :param column_names: A list of column names for the resulting DataFrame.\n",
    "    :return: A pandas DataFrame containing the query results.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Execute the SELECT query using the provided connection\n",
    "        cursor.execute(select_query)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        # Handle any query execution errors, print the error message, and close the cursor\n",
    "        print(\"Error: %s\" % error)\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    # Fetch all the query results into a list of tuples\n",
    "    tupples = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    # Create a pandas DataFrame from the list of tuples with the specified column names\n",
    "    df = pd.DataFrame(tupples, columns=column_names)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def connect(params_dic):\n",
    "    \"\"\"\n",
    "    Connect to the PostgreSQL database server using the provided connection parameters.\n",
    "        :param params_dic: A dictionary containing database connection parameters.\n",
    "        :return: A database connection object.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # Attempt to connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**params_dic)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        # Handle any connection errors, print the error message, and exit\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    print(\"Connection successful\")\n",
    "\n",
    "    # Return the database connection object\n",
    "    return conn\n",
    "\n",
    "\n",
    "\n",
    "def extract(query, param_dic):\n",
    "    # Passing the connection details for the PostgreSQL server\n",
    "    # Connect to the database using the provided connection parameters\n",
    "    conn = connect(param_dic)\n",
    "    # Execute the provided SQL query and retrieve the results as a DataFrame\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    # Print a message to indicate that the data extraction is complete\n",
    "    print(\"Data extract complete...\")\n",
    "    # Return the DataFrame containing the extracted data\n",
    "    return df\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    # Get a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    # Use a cursor for executing SQL commands\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        # Create a StringIO object to store the data in a CSV format\n",
    "        s_buf = StringIO()\n",
    "        # Create a CSV writer to write the data to the StringIO buffer\n",
    "        writer = csv.writer(s_buf)\n",
    "        # Write the data_iter (the data rows) to the StringIO buffer\n",
    "        writer.writerows(data_iter)\n",
    "        # Reset the buffer position to the beginning\n",
    "        s_buf.seek(0)\n",
    "        # Generate the list of column names as a comma-separated string\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        # Determine the full table name, including schema if it exists\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "        # Construct the SQL query for data insertion using the PostgreSQL COPY command\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        # Execute the COPY command with the data from the StringIO buffer\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "\n",
    "\n",
    "def close_connection(param_dic):\n",
    "    # Establish a connection to the PostgreSQL database using the provided parameters.\n",
    "    conn = psycopg2.connect(database=param_dic['database'],\n",
    "                            user=param_dic['user'],\n",
    "                            password=param_dic['password'],\n",
    "                            host=param_dic['host'],\n",
    "                            port=\"5432\")\n",
    "\n",
    "    # Print the 'closed' attribute of the connection (0 means open, 1 means closed).\n",
    "    print(conn.closed)\n",
    "    # Close the database connection to release resources.\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def upload(df, param_dic):\n",
    "    # Print a message to indicate the upload process has started.\n",
    "    print('Uploading to PostgreSQL...')\n",
    "    # Create a database engine using the provided connection parameters.\n",
    "    engine = create_engine('postgresql://' + param_dic['user'] + ':' + param_dic['password'] + '@' + param_dic['host'] + ':5432/' + param_dic['database'])\n",
    "    # Upload the DataFrame 'df' to the PostgreSQL database using the provided engine.\n",
    "    ''' - 'a_test' is the table name in the database.\n",
    "        - 'sandbox' is the schema where the table will be created.\n",
    "        - if_exists='replace' will replace the table if it already exists.\n",
    "        - index=False indicates not to include the DataFrame index in the database.'''\n",
    "    df.to_sql('a_test', engine, method=psql_insert_copy, schema='sandbox', if_exists='replace', index=False)\n",
    "    # Close the database connection using a separate function.\n",
    "    close_connection(param_dic)\n",
    "    # Print a message to indicate that the database connection has been closed.\n",
    "    print('Connection closed.')\n",
    "\n",
    "\n",
    "\n",
    "def run_query(query, param_dic):\n",
    "    # Establish a connection to the PostgreSQL database using the provided parameters.\n",
    "    conn = psycopg2.connect(database=param_dic['database'],\n",
    "                            user=param_dic['user'],\n",
    "                            password=param_dic['password'],\n",
    "                            host=param_dic['host'],\n",
    "                            port=\"5432\")\n",
    "\n",
    "    # Create a cursor object to interact with the database.\n",
    "    cursor = conn.cursor()\n",
    "    # Execute the SQL query provided as a parameter.\n",
    "    cursor.execute(query)\n",
    "    # Commit the transaction to save the changes to the database.\n",
    "    cursor.execute(\"COMMIT\")\n",
    "    # Close the database connection to release resources.\n",
    "    conn.close()\n",
    "\n",
    "def concat_csv_files(directory, extension='csv'):\n",
    "    \"\"\"\n",
    "    Concatenate multiple CSV files in a directory into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "        extension (str): File extension of the CSV files (default is 'csv').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame.\n",
    "\n",
    "    # Specify the directory and extension\n",
    "        directory_path = \"G:/.shortcut-targets-by-id/1uyENisO-y_QgU7JTbN4TWyLtow8ngXtZ/PJS/Coach\"\n",
    "        file_extension = 'csv'\n",
    "    # Call the function to concatenate CSV files\n",
    "        df = concat_csv_files(directory_path, file_extension)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)  # Change the working directory to the specified directory\n",
    "    all_filenames = [i for i in glob.glob(f'*.{extension}')]  # List of all CSV files\n",
    "\n",
    "    # Concatenate all CSV files into a single DataFrame\n",
    "    df = pd.concat([pd.read_csv(f, encoding='unicode_escape') for f in all_filenames], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahuOLynnApXT"
   },
   "source": [
    "### Overall sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tI1LH4InAt_j"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on CPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = 200\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    #defining the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=494,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', torchscript = True)\n",
    "\n",
    "\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "\n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "def overal_sentiment(df):\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "\n",
    "  model_save_name = 'bert_classifier.pt'\n",
    "  path = F\"/content/drive/Shared drives/CRM & Insight/Analysis/arun/Text_Analytics/{model_save_name}\"\n",
    "  bert_classifier.load_state_dict(torch.load(path))\n",
    "\n",
    "  test_inputs, test_masks = preprocessing_for_bert(df['response'].to_numpy())\n",
    "\n",
    "  # Create the DataLoader for our test set\n",
    "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "  # Compute predicted probabilities on the test set\n",
    "  probs = bert_predict(bert_classifier, test_dataloader)\n",
    "  classification = np.argmax(probs, axis = 1)\n",
    "\n",
    "  return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iQlq-xQDfy6"
   },
   "source": [
    "### Emotion detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uVEaKtzaDiGM"
   },
   "outputs": [],
   "source": [
    "def emotion_detector(df):\n",
    "  prob_df = df['response'].to_frame()\n",
    "  ds = Dataset.from_pandas(prob_df)\n",
    "\n",
    "  pipe = pipeline('text-classification', model=\"j-hartmann/emotion-english-distilroberta-base\", device = device)\n",
    "  results = pipe(KeyDataset(ds, \"response\"))\n",
    "\n",
    "  emotion_label = []\n",
    "  for idx, extracted_entities in enumerate(results):\n",
    "    emotion_label.append(extracted_entities['label'])\n",
    "\n",
    "  return emotion_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERy7GUzLRjtv"
   },
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dcnf8TdER7TJ"
   },
   "outputs": [],
   "source": [
    "def ner(df):\n",
    "  prob_df = df['response'].to_frame()\n",
    "  ds = Dataset.from_pandas(prob_df)\n",
    "\n",
    "  model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "  pipe = pipeline(task=\"ner\", model=model, tokenizer=tokenizer, device = device)\n",
    "  results = pipe(KeyDataset(ds, \"response\"))\n",
    "\n",
    "  original = []\n",
    "  temp_ent = []\n",
    "  entities = []\n",
    "\n",
    "  for idx, extracted_entities in enumerate(results):\n",
    "      temp_ent = []\n",
    "      # print('idx: ', idx)\n",
    "      # print(\"Original text:\\n{}\".format(ds[idx][\"response\"]))\n",
    "      # print(\"Extracted entities:\")\n",
    "      #original.append(dataset[idx][\"text\"])\n",
    "      for entity in extracted_entities:\n",
    "          temp_ent.append(entity)\n",
    "          # print(entity)\n",
    "      entities.append(temp_ent)\n",
    "\n",
    "  return entities\n",
    "\n",
    "def extraction(df):\n",
    "  df['entities'] = ner(df)\n",
    "  df2 = df.explode('entities')\n",
    "\n",
    "  df1 = df2[df2['entities'].isna() == True]\n",
    "  df2 = df2[df2['entities'].isna() == False]\n",
    "\n",
    "  #entity\n",
    "  df2['entity_type'] = df2['entities'].apply(lambda x:  x['entity'])\n",
    "  #start\n",
    "  df2['entity_start'] = df2['entities'].apply(lambda x:  x['start'])\n",
    "  #end\n",
    "  df2['entity_end'] = df2['entities'].apply(lambda x:  x['end'])\n",
    "  #word\n",
    "  df2['entity_word'] = df2['entities'].apply(lambda x:  x['word'])\n",
    "\n",
    "  df['Name_mask'] = np.where( df['ticket'].isin( df2[df2['entity_type'] == 'B-PER'] ), True, False )\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFNdnSRfXvX0"
   },
   "source": [
    "### Aspect based sentiment classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i58dRFF-Xxxz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# cleaning master function\n",
    "def clean_response(response, bigrams=False):\n",
    "    response = str(response).lower() # lower case\n",
    "    response = re.sub('['+my_punctuation + ']+', ' ', response) # strip punctuation\n",
    "    response = re.sub('\\s+', ' ', response) #remove double spacing\n",
    "    response = re.sub('([0-9]+)', '', response) # remove numbers\n",
    "    response_token_list = [word for word in response.split(' ')]\n",
    "                           # if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    response_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in response_token_list] # apply word rooter\n",
    "\n",
    "    lem_response = ' '.join(response_token_list)\n",
    "    return response, lem_response\n",
    "\n",
    "\n",
    "#lemmatizing taxonomy\n",
    "def lem_taxon(term):\n",
    "    term = str(term).lower()\n",
    "    term = word_rooter(term)\n",
    "    return term\n",
    "def match_both(response, non_lem, tax_list, tier2_list, tier1_list):\n",
    "    tier_match = []\n",
    "    tier_replace = ['replace', 'this', 'part']\n",
    "    words = response.split(' ')\n",
    "    non_lem_words = non_lem.split(' ')\n",
    "    for i, x in enumerate(tax_list):\n",
    "        for n,y in enumerate(words):\n",
    "          if x == y:\n",
    "            tier_match.append([non_lem_words[n], tier2_list[i], tier1_list[i]])\n",
    "\n",
    "    tier_match = [list(x) for x in set(tuple(x) for x in tier_match)]\n",
    "    return tier_match\n",
    "\n",
    "def apply_taxonomy(df):\n",
    "  df['tax_match'] = df.apply(lambda x: match_both(x['lem_response'], x['non_lem_response'], tax['Term'].to_list(), tax['Tier 2'].to_list(), tax['Tier 1'].to_list() ) , axis = 1)\n",
    "  df = df.explode('tax_match')\n",
    "\n",
    "  df['tax_match'] = df['tax_match'].fillna('No Taxonomy')\n",
    "  import math\n",
    "  #unpacking tax_match arrays into individual columns\n",
    "  tax_match = df['tax_match'].to_list()\n",
    "\n",
    "  term = []\n",
    "  tier2 = []\n",
    "  tier1 = []\n",
    "  clause = []\n",
    "\n",
    "  for i, c in enumerate(tax_match):\n",
    "    if c != 'No Taxonomy':\n",
    "      term.append(c[0])\n",
    "      tier2.append(c[1])\n",
    "      tier1.append(c[2])\n",
    "    else:\n",
    "      term.append(np.nan)\n",
    "      tier2.append(np.nan)\n",
    "      tier1.append(np.nan)\n",
    "\n",
    "  df['Term'], df['Tier_2'], df['Tier_1']= term, tier2, tier1\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0W0hh29ZFvz"
   },
   "source": [
    "### loading absa model for batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PiCpm4qEjmMK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example):\n",
    "    checkpoint = \"yangheng/deberta-v3-large-absa-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "    return tokenizer(example[\"response\"], example[\"Term\"], truncation=True)\n",
    "\n",
    "def absa_classification(df):\n",
    "  #packaging data appropriately\n",
    "  absa = df.dropna(subset = 'Term')\n",
    "  absa['Term'] = absa['Term'].apply(lambda x: text_preprocessing(x))\n",
    "  predict_ds = Dataset.from_pandas(absa[['response','Term']])\n",
    "\n",
    "  checkpoint = \"yangheng/deberta-v3-large-absa-v1.1\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "  #tokenizing dataset\n",
    "  tokenized_datasets = predict_ds.map(tokenize_function, batched=True)\n",
    "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "  #removing unnecessary columns\n",
    "  tokenized_datasets = tokenized_datasets.remove_columns([\"response\", \"Term\", \"__index_level_0__\"])\n",
    "  tokenized_datasets.set_format(\"torch\")\n",
    "  tokenized_datasets.column_names\n",
    "\n",
    "  predict_dataloader = DataLoader(\n",
    "      tokenized_datasets, shuffle=True, batch_size=8, collate_fn=data_collator\n",
    "  )\n",
    "\n",
    "  for batch in predict_dataloader:\n",
    "      break\n",
    "  {k: v.shape for k, v in batch.items()}\n",
    "\n",
    "  #downloading model from hugging face\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "  #updating fine tuned model parameters\n",
    "  path = r\"/content/drive/MyDrive/asba_classifier.pt\"\n",
    "  model.load_state_dict(torch.load(path))\n",
    "\n",
    "  #batch predictions\n",
    "  model.to(device)\n",
    "  all_logits = []\n",
    "  predictions = []\n",
    "  for batch in predict_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**batch)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      all_logits.append(logits)\n",
    "\n",
    "      prediction = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "      predictions.extend(prediction)\n",
    "\n",
    "  absa['term_sentiment'] = predictions\n",
    "  df = df.merge( absa[['ticket','Term','term_sentiment']], how = 'left', left_on = ['ticket','Term'], right_on = ['ticket','Term'] )\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GA1vhgIDq_fA"
   },
   "outputs": [],
   "source": [
    "\n",
    "#loading taxonomy\n",
    "taxonomy_path = r'/content/drive/Shared drives/CRM & Insight/Analysis/arun/Text_Analytics/taxonomy.csv'\n",
    "tax = pd.read_csv(taxonomy_path)\n",
    "#clean reason response\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@'\n",
    "\n",
    "\n",
    "tax['Term'] = tax['Term'].apply(lambda x: lem_taxon(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('pjs_all.csv', nrows = 200)\n",
    "df = df.dropna()\n",
    "\n",
    "df['nps'] = df['nps'].replace('10 (Extremely likely)',10)\n",
    "df['nps'] = df['nps'].replace('0 (Not at all likely)',0)\n",
    "df['nps'] = df['nps'].astype(int)\n",
    "\n",
    "#target variable will nps split into demoters, passives and promoters\n",
    "df['label'] = np.where(df['nps'] == 3,2,\n",
    "                  np.where(df['nps'] == 1,0,1))\n",
    "\n",
    "df['response'] = df['response'].apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMGEwTxsrRea",
    "outputId": "2a88ee79-22e6-4d3d-9f32-fdabf8f219cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "prob_df = df['response'].to_frame()\n",
    "\n",
    "ds = Dataset.from_pandas(prob_df)\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K6286yvWrXBY"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKhpY2pzt88S"
   },
   "outputs": [],
   "source": [
    "df['Overall sentiment'] = overal_sentiment(df)\n",
    "df['emotion'] = emotion_detector(df)\n",
    "\n",
    "#Using regex to mask phone numbers and emails\n",
    "#UK phone number\n",
    "phone_pattern = '((\\+44(\\s\\(0\\)\\s|\\s0\\s|\\s)?)|0)7\\d{3}(\\s)?\\d{6}'\n",
    "df['masked_response'] = df['response'].apply(lambda x: re.sub(phone_pattern, '[number_removed]',x ))\n",
    "\n",
    "#email address\n",
    "email_pattern = r'([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\\.[A-Z|a-z]{2,})+'\n",
    "df['masked_response'] = df['response'].apply(lambda x: re.sub(email_pattern, '[email_removed]',x ))\n",
    "\n",
    "df = extraction(df)\n",
    "\n",
    "df['non_lem_response'],df['lem_response'] = zip(*df['response'].apply(lambda x: clean_response(x)))\n",
    "df = apply_taxonomy(df)\n",
    "\n",
    "\n",
    "df = absa_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkZThDfg4uHd",
    "outputId": "f5d6673b-9142-40c6-9b1c-e9c0c67712e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to PostgreSQL...\n",
      "0\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_dic = {\n",
    "        \"host\"      : \"ec2-54-155-12-138.eu-west-1.compute.amazonaws.com\",\n",
    "        \"database\"  : \"d53ibcjuk3k509\",\n",
    "        \"user\"      : \"arun_nagrecha_nationalexpress_com\",\n",
    "        \"password\"  : \"p77cef9d11a8b16cc2e52423659dcc3e2d86393550ea7e0b6ac6b84050747d3b0\"\n",
    "    }\n",
    "\n",
    "upload(df,param_dic)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
